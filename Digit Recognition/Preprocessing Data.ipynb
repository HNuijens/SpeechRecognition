{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "298afd1c",
   "metadata": {},
   "source": [
    "<h1 style = \"font-size:3rem;color:darkcyan\"> Preprocessing Data</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67bbd1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c160157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how long audio files are\n",
    "def get_dataset_info(dataset_path, sample_rate=48000):\n",
    "    \n",
    "    min_length = max_length = average_length = 0\n",
    "    n_files = 0\n",
    "    \n",
    "    print('Going through all folders...')\n",
    "    \n",
    "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(root)):\n",
    "        if dirpath is not root:\n",
    "            for file in filenames:\n",
    "                n_files +=1\n",
    "                \n",
    "                # load audio\n",
    "                file_path = os.path.join(dirpath + '/' + file)\n",
    "                audio_file, _ = librosa.load(file_path, sr = sample_rate, mono=True)\n",
    "              \n",
    "                audio_length = len(audio_file)\n",
    "                \n",
    "                average_length += audio_length\n",
    "                \n",
    "                if min_length > audio_length:\n",
    "                    min_length = audio_length\n",
    "                    \n",
    "                if  max_length < audio_length:\n",
    "                    max_length = audio_length\n",
    "    \n",
    "    average_length = average_length / n_files\n",
    "    print('The average audio length = {}, min length = {}, and max length = {}'.format(average_length/sample_rate, min_length/sample_rate, max_length/sample_rate))\n",
    "    return average_length/sample_rate, min_length/sample_rate, max_length/sample_rate  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40b0264d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through all folders...\n",
      "The average audio length = 0.6425932402777778, min length = 0.0, and max length = 0.9999583333333333\n"
     ]
    }
   ],
   "source": [
    "average_duration, min_duration, max_duration = get_dataset_info('../../../Datasets/Speech/Digits/AudioMNIST/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b58fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset_path, json_path, audio_duration, n_mfcc = 13, n_fft = 2048, hop_size = 512, sample_rate=48000):\n",
    "    data  = {\n",
    "        'mappings' : [],  # corresponding digit\n",
    "        'labels' : [],    # corresponding number\n",
    "        'MFCC' : [],      # extracted mfcc\n",
    "        'filenames' : []  # original filenames\n",
    "     }\n",
    "   \n",
    "    n_samples_in_audio_file = int(sample_rate * audio_duration)\n",
    "    \n",
    "    data['mappings'] = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "    \n",
    "    n_folders = len(os.listdir(dataset_path))\n",
    "    \n",
    "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(root)):\n",
    "        if dirpath is not root:\n",
    "            print(f'processing folder {i} out of {n_folders}')\n",
    "            \n",
    "            for file in filenames:\n",
    "                # load audio\n",
    "                file_path = os.path.join(dirpath + '/' + file)\n",
    "                audio_file, _ = librosa.load(file_path, sr = sample_rate, mono=True)\n",
    "                \n",
    "                # ignore if it contains too few samples\n",
    "                if len(audio_file) >= n_samples_in_audio_file:\n",
    "                    \n",
    "                    audio_file = audio_file[:n_samples_in_audio_file]\n",
    "                    \n",
    "                    # extract mfccs\n",
    "                    mfcc = librosa.feature.mfcc(y = audio_file,\n",
    "                                                sr = sample_rate,\n",
    "                                                n_mfcc = n_mfcc,\n",
    "                                                n_fft = n_fft,\n",
    "                                                hop_length = hop_size)\n",
    "\n",
    "                    # store data\n",
    "                    data['labels'].append(int(file[0]))\n",
    "                    data['MFCC'].append(mfcc.T.tolist()) \n",
    "                    data['filenames'].append(file)        \n",
    "    \n",
    "    print(f'Saving dataset as {json_path}...')                \n",
    "    with open(json_path, 'w') as fp:\n",
    "        json.dump(data, fp, indent = 4)\n",
    "    print(f'Done saving ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09c25585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing folder 1 out of 60\n",
      "processing folder 2 out of 60\n",
      "processing folder 3 out of 60\n",
      "processing folder 4 out of 60\n",
      "processing folder 5 out of 60\n",
      "processing folder 6 out of 60\n",
      "processing folder 7 out of 60\n",
      "processing folder 8 out of 60\n",
      "processing folder 9 out of 60\n",
      "processing folder 10 out of 60\n",
      "processing folder 11 out of 60\n",
      "processing folder 12 out of 60\n",
      "processing folder 13 out of 60\n",
      "processing folder 14 out of 60\n",
      "processing folder 15 out of 60\n",
      "processing folder 16 out of 60\n",
      "processing folder 17 out of 60\n",
      "processing folder 18 out of 60\n",
      "processing folder 19 out of 60\n",
      "processing folder 20 out of 60\n",
      "processing folder 21 out of 60\n",
      "processing folder 22 out of 60\n",
      "processing folder 23 out of 60\n",
      "processing folder 24 out of 60\n",
      "processing folder 25 out of 60\n",
      "processing folder 26 out of 60\n",
      "processing folder 27 out of 60\n",
      "processing folder 28 out of 60\n",
      "processing folder 29 out of 60\n",
      "processing folder 30 out of 60\n",
      "processing folder 31 out of 60\n",
      "processing folder 32 out of 60\n",
      "processing folder 33 out of 60\n",
      "processing folder 34 out of 60\n",
      "processing folder 35 out of 60\n",
      "processing folder 36 out of 60\n",
      "processing folder 37 out of 60\n",
      "processing folder 38 out of 60\n",
      "processing folder 39 out of 60\n",
      "processing folder 40 out of 60\n",
      "processing folder 41 out of 60\n",
      "processing folder 42 out of 60\n",
      "processing folder 43 out of 60\n",
      "processing folder 44 out of 60\n",
      "processing folder 45 out of 60\n",
      "processing folder 46 out of 60\n",
      "processing folder 47 out of 60\n",
      "processing folder 48 out of 60\n",
      "processing folder 49 out of 60\n",
      "processing folder 50 out of 60\n",
      "processing folder 51 out of 60\n",
      "processing folder 52 out of 60\n",
      "processing folder 53 out of 60\n",
      "processing folder 54 out of 60\n",
      "processing folder 55 out of 60\n",
      "processing folder 56 out of 60\n",
      "processing folder 57 out of 60\n",
      "processing folder 58 out of 60\n",
      "processing folder 59 out of 60\n",
      "processing folder 60 out of 60\n",
      "Saving dataset as data.json...\n",
      "Done saving \n"
     ]
    }
   ],
   "source": [
    "root = '../../../Datasets/Speech/Digits/AudioMNIST/data/'\n",
    "preprocess_dataset(root, 'data.json', audio_duration = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5026f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset_path, json_path, audio_duration, n_mfcc = 13, n_fft = 2048, hop_size = 512, sample_rate=48000):\n",
    "    data  = {\n",
    "        'mappings' : [],  # corresponding genre\n",
    "        'labels' : [],    # corresponding number\n",
    "        'MFCC' : [],      # extracted mfcc\n",
    "        'filenames' : []  # original filenames\n",
    "     }\n",
    "   \n",
    "    n_samples_in_audio_file = int(sample_rate * audio_duration)\n",
    "    \n",
    "    data['mappings'].append(['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine'])\n",
    "    \n",
    "    n_folders = len(os.listdir(dataset_path))\n",
    "    \n",
    "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(root)):\n",
    "        if dirpath is not root:\n",
    "            print(f'processing folder {i} out of {n_folders}')\n",
    "            \n",
    "            for file in filenames:\n",
    "                # load audio\n",
    "                file_path = os.path.join(dirpath + '/' + file)\n",
    "                audio_file, _ = librosa.load(file_path, sr = sample_rate, mono=True)\n",
    "                \n",
    "                # zero pad signal if too small\n",
    "                if len(audio_file) < n_samples_in_audio_file:\n",
    "                    zero_pad = np.zeros(n_samples_in_audio_file - len(audio_file))\n",
    "                    audio_file.append(zero_pad)\n",
    "\n",
    "                # extract mfccs\n",
    "                mfcc = librosa.feature.mfcc(y = audio_file,\n",
    "                                            sr = sample_rate,\n",
    "                                            n_mfcc = n_mfcc,\n",
    "                                            n_fft = n_fft,\n",
    "                                            hop_length = hop_size)\n",
    "\n",
    "                # store data\n",
    "                data['labels'].append(i-1)\n",
    "                data['MFCC'].append(mfcc.T.tolist()) \n",
    "                data['filenames'].append(file)        \n",
    "    \n",
    "    print(f'Saving dataset as {json_path}...')                \n",
    "    with open(json_path, 'w') as fp:\n",
    "        json.dump(data, fp, indent = 4)\n",
    "    print(f'Done saving ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "717e6ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing folder 1 out of 60\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../../Datasets/Speech/Digits/AudioMNIST/data/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mpreprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_duration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_duration\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [47]\u001b[0m, in \u001b[0;36mpreprocess_dataset\u001b[1;34m(dataset_path, json_path, audio_duration, n_mfcc, n_fft, hop_size, sample_rate)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(audio_file) \u001b[38;5;241m<\u001b[39m n_samples_in_audio_file:\n\u001b[0;32m     26\u001b[0m     zero_pad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(n_samples_in_audio_file \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(audio_file))\n\u001b[1;32m---> 27\u001b[0m     \u001b[43maudio_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(zero_pad)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# extract mfccs\u001b[39;00m\n\u001b[0;32m     30\u001b[0m mfcc \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mmfcc(y \u001b[38;5;241m=\u001b[39m audio_file,\n\u001b[0;32m     31\u001b[0m                             sr \u001b[38;5;241m=\u001b[39m sample_rate,\n\u001b[0;32m     32\u001b[0m                             n_mfcc \u001b[38;5;241m=\u001b[39m n_mfcc,\n\u001b[0;32m     33\u001b[0m                             n_fft \u001b[38;5;241m=\u001b[39m n_fft,\n\u001b[0;32m     34\u001b[0m                             hop_length \u001b[38;5;241m=\u001b[39m hop_size)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "root = '../../../Datasets/Speech/Digits/AudioMNIST/data/'\n",
    "preprocess_dataset(root, 'data.json', audio_duration = max_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cc0ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
