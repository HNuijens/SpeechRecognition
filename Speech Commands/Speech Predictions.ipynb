{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f77f19d",
   "metadata": {},
   "source": [
    "<h1 style = \"font-size:3rem;color:darkcyan\"> Make Speech Predictions</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b949621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5c99477",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keyword_Recognition:\n",
    "    \n",
    "    def __init__(self, model_path, sample_rate = 22500, audio_dur = 1, hop_length = 512, n_mfcc = 13, n_fft = 2048):\n",
    "    \n",
    "        self.model = keras.models.load_model(model_path)\n",
    "        \n",
    "        self._mappings = [\n",
    "            \"backward\",\n",
    "            \"bed\",\n",
    "            \"bird\",\n",
    "            \"cat\",\n",
    "            \"dog\",\n",
    "            \"down\",\n",
    "            \"eight\",\n",
    "            \"five\",\n",
    "            \"follow\",\n",
    "            \"forward\",\n",
    "            \"four\",\n",
    "            \"go\",\n",
    "            \"happy\",\n",
    "            \"house\",\n",
    "            \"learn\",\n",
    "            \"left\",\n",
    "            \"marvin\",\n",
    "            \"nine\",\n",
    "            \"no\",\n",
    "            \"off\",\n",
    "            \"on\",\n",
    "            \"one\",\n",
    "            \"right\",\n",
    "            \"seven\",\n",
    "            \"sheila\",\n",
    "            \"six\",\n",
    "            \"stop\",\n",
    "            \"three\",\n",
    "            \"tree\",\n",
    "            \"two\",\n",
    "            \"up\",\n",
    "            \"visual\",\n",
    "            \"wow\",\n",
    "            \"yes\",\n",
    "            \"zero\"\n",
    "        ]\n",
    "        \n",
    "        self.n_samples = int(audio_dur * sample_rate)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "    def _preprocess(self, file_path):\n",
    "        audio_data, _ = librosa.load(file_path, sr= self.sample_rate, mono=True)\n",
    "\n",
    "        # ignore < 1 sec of audio\n",
    "        if len(audio_data) >= self.n_samples:\n",
    "\n",
    "            # crop audio when larger\n",
    "            if len(audio_data > self.n_samples):\n",
    "                audio_data = audio_data[:self.n_samples] \n",
    "\n",
    "            # extract the MFCC\n",
    "            mfcc = librosa.feature.mfcc(y = audio_data, \n",
    "                                                sr = self.sample_rate, \n",
    "                                                n_mfcc = self.mfcc, \n",
    "                                                n_fft = self.n_fft, \n",
    "                                                hop_length = self.hop_length)\n",
    "\n",
    "            return mfcc.T\n",
    "        \n",
    "        print('Audiofile has insufficient length')\n",
    "        return \n",
    "        \n",
    "    def predict(self, file_path):\n",
    "        \n",
    "        # extract MFCCs\n",
    "        MFCCs = _preprocess(file_path)\n",
    "\n",
    "        # add axis for CNN network\n",
    "        MFCCs = MFCCs[..., np.newaxis]\n",
    "\n",
    "        # make prediction\n",
    "        y_pred = self.model.predict(MFCCs)\n",
    "\n",
    "        # get index of higest probability\n",
    "        pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # print the label\n",
    "        print(self.mappings[pred_class])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66117f30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keyword_recognition = Keyword_Recognition(model_path = 'model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34030a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '_preprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m audio_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../../Datasets/Speech/Digits/AudioMNIST/data/04/0_04_4.wav\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mkeyword_recognition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [35]\u001b[0m, in \u001b[0;36mKeyword_Recognition.predict\u001b[1;34m(self, file_path)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicting\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# extract MFCCs\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m MFCCs \u001b[38;5;241m=\u001b[39m \u001b[43m_preprocess\u001b[49m(file_path)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# add axis for CNN network\u001b[39;00m\n\u001b[0;32m     79\u001b[0m MFCCs \u001b[38;5;241m=\u001b[39m MFCCs[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, np\u001b[38;5;241m.\u001b[39mnewaxis]\n",
      "\u001b[1;31mNameError\u001b[0m: name '_preprocess' is not defined"
     ]
    }
   ],
   "source": [
    "audio_file_path = '../../../Datasets/Speech/Digits/AudioMNIST/data/04/0_04_4.wav'\n",
    "keyword_recognition.predict(audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1354fb60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
